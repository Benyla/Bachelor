#!/usr/bin/env python3
"""
compute_fid.py

Compute Fr√©chet Inception Distance (FID) between real validation set samples and
samples generated by a trained VAE model, using the pytorch-fid library's InceptionV3
pool3 features for standard benchmarking.
"""
import argparse
import os
import torch
import torch.nn.functional as F
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
from torchvision import transforms

from pytorch_fid.inception import InceptionV3
from pytorch_fid.fid_score import calculate_frechet_distance

from src.utils.config_loader import load_config
from src.utils.data_loader import get_data, SingleCellDataset
from src.models.VAE import VAE


def get_activations(data_loader, model, device):
    """
    Extract pooled InceptionV3 pool3 activations (2048-D) for all samples in data_loader.
    """
    model.eval()
    activations = []
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])

    with torch.no_grad():
        for batch in data_loader:
            # unpack if needed
            if isinstance(batch, (list, tuple)):
                batch = batch[0]
            batch = batch.to(device)
            # convert to 3-channel if single-channel
            if batch.dim() == 4 and batch.shape[1] == 1:
                batch = batch.repeat(1, 3, 1, 1)
            # resize to 299x299
            batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)
            # normalize
            batch = normalize(batch)
            # forward through Inception
            pred = model(batch)[0]  # list with one element: pool3
            # flatten spatial dims
            pred = pred.squeeze(3).squeeze(2)
            activations.append(pred.cpu().numpy())
    return np.concatenate(activations, axis=0)


def calculate_activation_statistics(activations):
    """
    Compute mean and covariance of activations.
    """
    mu = np.mean(activations, axis=0)
    sigma = np.cov(activations, rowvar=False)
    return mu, sigma


def generate_images(model, num_samples, latent_dim, device, batch_size):
    """
    Generate images by sampling from the VAE's latent space.
    """
    model.eval()
    images = []
    with torch.no_grad():
        for i in range(0, num_samples, batch_size):
            curr = min(batch_size, num_samples - i)
            z = torch.randn(curr, latent_dim).to(device)
            recon = model.decode(z)
            images.append(recon.cpu())
    return torch.cat(images, dim=0)


def main():
    parser = argparse.ArgumentParser(description="Compute FID for VAE-generated vs real samples using pytorch-fid")
    parser.add_argument('--config', type=str, required=True, help='Path to YAML config')
    parser.add_argument('--model-path', type=str, required=True, help='Path to VAE checkpoint (.pth)')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size for feature extraction')
    parser.add_argument('--num-samples', type=int, default=None,
                        help='Number of samples to use (both real and generated); defaults to full val set')
    parser.add_argument('--output', type=str, default='experiments/fid', help='Directory to save FID score')
    args = parser.parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Load config and model
    config = load_config(args.config)
    config['model']['checkpoint_path'] = args.model_path
    vae = VAE(
        in_channels=config['model']['in_channels'],
        latent_dim=config['model']['latent_dim']
    ).to(device)
    ckpt = torch.load(args.model_path, map_location=device)
    vae.load_state_dict(ckpt['model_state_dict'])

    # Prepare data loaders
    _, val_files, _ = get_data()
    real_dataset = SingleCellDataset(val_files)
    real_loader = DataLoader(real_dataset, batch_size=args.batch_size,
                             shuffle=False, drop_last=False)

    # Determine sample count
    # Extract real activations
    # Initialize InceptionV3 for pool3 features
    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]
    inception = InceptionV3([block_idx]).to(device)

    real_acts = get_activations(real_loader, inception, device)
    num_real = real_acts.shape[0]
    num_samples = args.num_samples or num_real

    # Subsample real if needed
    if num_samples < num_real:
        idx = np.random.choice(num_real, num_samples, replace=False)
        real_acts = real_acts[idx]

    # Generate images and activations for generated samples
    gen_images = generate_images(vae, num_samples, config['model']['latent_dim'], device, args.batch_size)
    gen_loader = DataLoader(TensorDataset(gen_images), batch_size=args.batch_size)
    gen_acts = get_activations(gen_loader, inception, device)

    # Compute statistics
    mu_real, sigma_real = calculate_activation_statistics(real_acts)
    mu_gen, sigma_gen = calculate_activation_statistics(gen_acts)

    # Compute FID
    fid_value = calculate_frechet_distance(mu_real, sigma_real, mu_gen, sigma_gen)

    # Save and report
    os.makedirs(args.output, exist_ok=True)
    out_file = os.path.join(args.output, 'fid_score.txt')
    with open(out_file, 'w') as f:
        f.write(f'{fid_value:.6f}\n')

    print(f'[INFO] FID score: {fid_value:.6f}')
    print(f'[INFO] Saved FID score to {out_file}')

if __name__ == '__main__':
    main()
